{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[*]\") \\\n",
    ".config(\"spark.driver.memory\", \"6g\") \\\n",
    ".getOrCreate()\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "25/06/14 07:17:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "! pwd\n",
    "/\n",
    "df = spark.read.format(\"parquet\").load(\"file:///part-00096-5bd1507a-125d-4ec6-8794-3ed02e45a45d.c000.snappy.parquet\")\n",
    "df.limit(5).toPandas()\n",
    "VendorID\ttpep_pickup_datetime\ttpep_dropoff_datetime\tpassenger_count\ttrip_distance\tRatecodeID\tstore_and_fwd_flag\tPULocationID\tDOLocationID\tpayment_type\tfare_amount\textra\tmta_tax\ttip_amount\ttolls_amount\timprovement_surcharge\ttotal_amount\tcongestion_surcharge\tAirport_fee\n",
    "0\t2\t2024-01-04 23:38:18\t2024-01-05 00:05:16\t1.0\t4.47\t1.0\tN\t163\t239\t2\t25.4\t1.0\t0.5\t0.00\t0.0\t1.0\t30.40\t2.5\t0.00\n",
    "1\t2\t2024-01-04 02:21:06\t2024-01-05 00:00:00\t1.0\t12.86\t1.0\tN\t70\t219\t1\t49.9\t6.0\t0.5\t5.74\t0.0\t1.0\t63.14\t0.0\t0.00\n",
    "2\t2\t2024-01-04 08:56:48\t2024-01-05 00:00:00\t1.0\t11.49\t1.0\tN\t132\t173\t2\t44.3\t0.0\t0.5\t0.00\t0.0\t1.0\t47.55\t0.0\t1.75\n",
    "3\t2\t2024-01-04 08:16:22\t2024-01-05 08:02:11\t1.0\t4.48\t1.0\tN\t237\t158\t1\t24.7\t0.0\t0.5\t2.87\t0.0\t1.0\t31.57\t2.5\t0.00\n",
    "4\t2\t2024-01-04 08:55:16\t2024-01-05 00:00:00\t2.0\t11.43\t1.0\tN\t243\t242\t2\t48.5\t0.0\t0.5\t0.00\t0.0\t1.0\t50.00\t0.0\t0.00\n",
    "DimTime:\n",
    "TimeID\n",
    "Date\n",
    "DayOfMonth\n",
    "WeekOfYear\n",
    "Month\n",
    "Year\n",
    "Hour\n",
    "Minute\n",
    "Second\n",
    "def write_dimtime(spark: SparkSession, table_name: str = \"dimtime\", start_date: str = \"2022-01-01\",\n",
    "                  end_date: str = \"2024-12-31\"):\n",
    "    try:\n",
    "        # Convert string dates to timestamp\n",
    "        start_ts = F.to_timestamp(F.lit(start_date))\n",
    "        end_ts = F.to_timestamp(F.lit(end_date))\n",
    "\n",
    "        # Generate minute-level timestamps using Spark\n",
    "        dimtime_df = spark.sql(f\"\"\"\n",
    "            SELECT explode(sequence(\n",
    "                to_timestamp('{start_date}'), \n",
    "                to_timestamp('{end_date}'), \n",
    "                interval 1 minute\n",
    "            )) as minutes\n",
    "        \"\"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error generating DataFrame: {str(e)}\")\n",
    "\n",
    "    dimtime_df2 = (dimtime_df.withColumn(\"unix_time_minutes\", F.round(F.unix_timestamp(F.col(\"minutes\")) / 60) * 60)\n",
    "                   .withColumn(\"timeid\", F.col(\"unix_time_minutes\").cast(\"integer\"))\n",
    "                   .withColumn(\"Date\", F.to_date(\"minutes\"))\n",
    "                   .withColumn(\"dayofmonth\", F.dayofmonth(\"minutes\"))\n",
    "                   .withColumn(\"weekofyear\", ((F.dayofyear(\"minutes\") - 1) / 7 + 1).cast(\"int\"))\n",
    "                   .withColumn(\"month\", F.month(\"minutes\").cast(\"smallint\"))\n",
    "                   .withColumn(\"year\", F.year(\"minutes\").cast(\"smallint\"))\n",
    "                   .withColumn(\"minute\", F.minute(\"minutes\").cast(\"smallint\"))\n",
    "                   .withColumn(\"second\", F.second(\"minutes\").cast(\"smallint\"))\n",
    "                   ).drop(\"minutes\", \"unix_time_minutes\")\n",
    "    return dimtime_df2\n",
    "dimtime_df = write_dimtime(spark, \"dimtime\", \"2022-01-01\", \"2025-12-31\")\n",
    "dimtime_df.sample(fraction=0.1).orderBy(F.rand()).limit(20).toPandas()\n",
    "25/06/14 07:35:49 WARN DAGScheduler: Broadcasting large task binary with size 16.4 MiB\n",
    "timeid\tDate\tdayofmonth\tweekofyear\tmonth\tyear\tminute\tsecond\n",
    "0\t1701977340\t2023-12-07\t7\t49\t12\t2023\t29\t0\n",
    "1\t1740542640\t2025-02-26\t26\t9\t2\t2025\t4\t0\n",
    "2\t1665980400\t2022-10-17\t17\t42\t10\t2022\t20\t0\n",
    "3\t1696747680\t2023-10-08\t8\t41\t10\t2023\t48\t0\n",
    "4\t1642665180\t2022-01-20\t20\t3\t1\t2022\t53\t0\n",
    "5\t1658107920\t2022-07-18\t18\t29\t7\t2022\t32\t0\n",
    "6\t1760480760\t2025-10-14\t14\t41\t10\t2025\t26\t0\n",
    "7\t1751737080\t2025-07-05\t5\t27\t7\t2025\t38\t0\n",
    "8\t1641468540\t2022-01-06\t6\t1\t1\t2022\t29\t0\n",
    "9\t1702381380\t2023-12-12\t12\t50\t12\t2023\t43\t0\n",
    "10\t1716225060\t2024-05-20\t20\t21\t5\t2024\t11\t0\n",
    "11\t1745759760\t2025-04-27\t27\t17\t4\t2025\t16\t0\n",
    "12\t1756440240\t2025-08-29\t29\t35\t8\t2025\t4\t0\n",
    "13\t1654156620\t2022-06-02\t2\t22\t6\t2022\t57\t0\n",
    "14\t1672216980\t2022-12-28\t28\t52\t12\t2022\t43\t0\n",
    "15\t1713457320\t2024-04-18\t18\t16\t4\t2024\t22\t0\n",
    "16\t1741042380\t2025-03-03\t3\t9\t3\t2025\t53\t0\n",
    "17\t1712688660\t2024-04-09\t9\t15\t4\t2024\t51\t0\n",
    "18\t1663645260\t2022-09-20\t20\t38\t9\t2022\t41\t0\n",
    "19\t1756551960\t2025-08-30\t30\t35\t8\t2025\t6\t0\n",
    "dimtime_df.count()\n",
    "25/06/14 07:36:15 WARN DAGScheduler: Broadcasting large task binary with size 16.4 MiB\n",
    "2102401\n",
    "DimLocation:\n",
    "LocationID\n",
    "Borough\n",
    "Zone\n",
    "ServiceZone\n",
    "! wget https://raw.githubusercontent.com/erkansirin78/datasets/00583d127f0cc780bc85c9a033d9e895b10bc4a3/nyc_taxi_yellow_trip_raw/taxi_zone_lookup.csv\n",
    "--2025-06-14 07:41:11--  https://raw.githubusercontent.com/erkansirin78/datasets/00583d127f0cc780bc85c9a033d9e895b10bc4a3/nyc_taxi_yellow_trip_raw/taxi_zone_lookup.csv\n",
    "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
    "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 12065 (12K) [text/plain]\n",
    "Saving to: ‘taxi_zone_lookup.csv’\n",
    "\n",
    "taxi_zone_lookup.cs 100%[===================>]  11.78K  --.-KB/s    in 0s      \n",
    "\n",
    "2025-06-14 07:41:11 (36.5 MB/s) - ‘taxi_zone_lookup.csv’ saved [12065/12065]\n",
    "\n",
    " \n",
    "def write_dimlocation(spark: SparkSession, path: str = None, table_name: str = \"dimlocation\"):\n",
    "    # Read txi zones\n",
    "    taxi_zones_df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "    return taxi_zones_df\n",
    "taxi_zones_df = write_dimlocation(spark=spark, path=\"/taxi_zone_lookup.csv\")\n",
    "taxi_zones_df.limit(10).toPandas()\n",
    "LocationID\tBorough\tZone\tservice_zone\n",
    "0\t1\tEWR\tNewark Airport\tEWR\n",
    "1\t2\tQueens\tJamaica Bay\tBoro Zone\n",
    "2\t3\tBronx\tAllerton/Pelham Gardens\tBoro Zone\n",
    "3\t4\tManhattan\tAlphabet City\tYellow Zone\n",
    "4\t5\tStaten Island\tArden Heights\tBoro Zone\n",
    "5\t6\tStaten Island\tArrochar/Fort Wadsworth\tBoro Zone\n",
    "6\t7\tQueens\tAstoria\tBoro Zone\n",
    "7\t8\tQueens\tAstoria Park\tBoro Zone\n",
    "8\t9\tQueens\tAuburndale\tBoro Zone\n",
    "9\t10\tQueens\tBaisley Park\tBoro Zone\n",
    "df.join(taxi_zones_df, df.PULocationID == taxi_zones_df.LocationID, how='inner').limit(10).toPandas()\n",
    "VendorID\ttpep_pickup_datetime\ttpep_dropoff_datetime\tpassenger_count\ttrip_distance\tRatecodeID\tstore_and_fwd_flag\tPULocationID\tDOLocationID\tpayment_type\t...\ttip_amount\ttolls_amount\timprovement_surcharge\ttotal_amount\tcongestion_surcharge\tAirport_fee\tLocationID\tBorough\tZone\tservice_zone\n",
    "0\t2\t2024-01-04 23:38:18\t2024-01-05 00:05:16\t1.0\t4.47\t1.0\tN\t163\t239\t2\t...\t0.00\t0.0\t1.0\t30.40\t2.5\t0.00\t163\tManhattan\tMidtown North\tYellow Zone\n",
    "1\t2\t2024-01-04 02:21:06\t2024-01-05 00:00:00\t1.0\t12.86\t1.0\tN\t70\t219\t1\t...\t5.74\t0.0\t1.0\t63.14\t0.0\t0.00\t70\tQueens\tEast Elmhurst\tBoro Zone\n",
    "2\t2\t2024-01-04 08:56:48\t2024-01-05 00:00:00\t1.0\t11.49\t1.0\tN\t132\t173\t2\t...\t0.00\t0.0\t1.0\t47.55\t0.0\t1.75\t132\tQueens\tJFK Airport\tAirports\n",
    "3\t2\t2024-01-04 08:16:22\t2024-01-05 08:02:11\t1.0\t4.48\t1.0\tN\t237\t158\t1\t...\t2.87\t0.0\t1.0\t31.57\t2.5\t0.00\t237\tManhattan\tUpper East Side South\tYellow Zone\n",
    "4\t2\t2024-01-04 08:55:16\t2024-01-05 00:00:00\t2.0\t11.43\t1.0\tN\t243\t242\t2\t...\t0.00\t0.0\t1.0\t50.00\t0.0\t0.00\t243\tManhattan\tWashington Heights North\tBoro Zone\n",
    "5\t2\t2024-01-04 09:14:56\t2024-01-05 00:00:00\t6.0\t13.60\t1.0\tN\t138\t88\t1\t...\t10.04\t0.0\t1.0\t76.99\t2.5\t1.75\t138\tQueens\tLaGuardia Airport\tAirports\n",
    "6\t2\t2024-01-04 10:03:30\t2024-01-05 09:04:46\t1.0\t2.14\t1.0\tN\t230\t142\t1\t...\t0.00\t0.0\t1.0\t26.60\t2.5\t0.00\t230\tManhattan\tTimes Sq/Theatre District\tYellow Zone\n",
    "7\t2\t2024-01-04 11:08:33\t2024-01-05 11:00:59\t1.0\t1.70\t1.0\tN\t186\t229\t1\t...\t1.96\t0.0\t1.0\t21.56\t2.5\t0.00\t186\tManhattan\tPenn Station/Madison Sq West\tYellow Zone\n",
    "8\t2\t2024-01-04 11:54:43\t2024-01-05 11:22:31\t6.0\t1.74\t1.0\tN\t238\t143\t1\t...\t3.50\t0.0\t1.0\t21.00\t2.5\t0.00\t238\tManhattan\tUpper West Side North\tYellow Zone\n",
    "9\t2\t2024-01-04 11:58:57\t2024-01-05 11:47:22\t1.0\t5.45\t1.0\tN\t261\t163\t1\t...\t3.43\t0.0\t1.0\t37.73\t2.5\t0.00\t261\tManhattan\tWorld Trade Center\tYellow Zone\n",
    "10 rows × 23 columns\n",
    "\n",
    "DimPayment:\n",
    "PaymentTypeID\n",
    "PaymentType\n",
    "df.groupBy(\"payment_type\").count().show()\n",
    "+------------+-----+\n",
    "|payment_type|count|\n",
    "+------------+-----+\n",
    "|           1|79376|\n",
    "|           3|  758|\n",
    "|           2|17911|\n",
    "|           4| 1741|\n",
    "+------------+-----+\n",
    "\n",
    "def write_pyment_type(spark):\n",
    "    data = [\n",
    "        (1, \"Credit card\"),\n",
    "        (2, \"Cash\"),\n",
    "        (3, \"No charge\"),\n",
    "        (4, \"Dispute\"),\n",
    "        (5, \"Unknown\"),\n",
    "        (6, \"Voided trip\")\n",
    "    ]\n",
    "\n",
    "    # Define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"PaymentTypeID\", IntegerType(), False),\n",
    "        StructField(\"PaymentType\", StringType(), False)\n",
    "    ])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    payment_type_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    return payment_type_df\n",
    "payment_type_df = write_pyment_type(spark)\n",
    "payment_type_df.show()\n",
    "                                                                                \n",
    "+-------------+-----------+\n",
    "|PaymentTypeID|PaymentType|\n",
    "+-------------+-----------+\n",
    "|            1|Credit card|\n",
    "|            2|       Cash|\n",
    "|            3|  No charge|\n",
    "|            4|    Dispute|\n",
    "|            5|    Unknown|\n",
    "|            6|Voided trip|\n",
    "+-------------+-----------+\n",
    "\n",
    "DimRateCode:\n",
    "RateCodeID\n",
    "RateDescription\n",
    "def write_ratecode(spark: SparkSession):\n",
    "    # Data extracted from the image\n",
    "    data = [\n",
    "        (1, \"Standard rate\"),\n",
    "        (2, \"JFK\"),\n",
    "        (3, \"Newark\"),\n",
    "        (4, \"Nassau or Westchester\"),\n",
    "        (5, \"Negotiated fare\"),\n",
    "        (6, \"Group ride\"),\n",
    "        (99, \"Unknown\")\n",
    "    ]\n",
    "\n",
    "    # Define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"RateCodeID\", IntegerType(), False),\n",
    "        StructField(\"RateDescription\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    return df\n",
    "ratecode_df = write_ratecode(spark)\n",
    "ratecode_df.show()\n",
    "+----------+--------------------+\n",
    "|RateCodeID|     RateDescription|\n",
    "+----------+--------------------+\n",
    "|         1|       Standard rate|\n",
    "|         2|                 JFK|\n",
    "|         3|              Newark|\n",
    "|         4|Nassau or Westche...|\n",
    "|         5|     Negotiated fare|\n",
    "|         6|          Group ride|\n",
    "|        99|             Unknown|\n",
    "+----------+--------------------+\n",
    "\n",
    "df.groupBy(\"RatecodeID\").count().show()\n",
    "+----------+-----+\n",
    "|RatecodeID|count|\n",
    "+----------+-----+\n",
    "|       1.0|94494|\n",
    "|       4.0|  188|\n",
    "|       3.0|  267|\n",
    "|       2.0| 3299|\n",
    "|      99.0|  917|\n",
    "|       5.0|  621|\n",
    "+----------+-----+\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
